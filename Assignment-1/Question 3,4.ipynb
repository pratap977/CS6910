{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question 3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratap977/CS6910/blob/main/Assignment-1/Question%203%2C4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "yh3rpitmV-BG",
        "outputId": "d728cb1a-0242-4eb0-e5ee-848f3b2bf581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 890 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 921 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 993 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7 MB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=765913a604dc089fd9392b33baf2dea91172944784e317d682c5578303cf59a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from  matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((xtrain, ytrain), (xtest, ytest)) = fashion_mnist.load_data()\n",
        "\n",
        "items=[\"T-shirt/top\", \"Trouser\",\"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\",\"Ankle boot\"]\n",
        "ftrx=np.zeros((60000,784))\n",
        "ftry=np.zeros((60000,10))\n",
        "ftex=np.zeros((10000,784))\n",
        "ftey=np.zeros((10000,10))\n",
        "for i in range(len(xtrain)):\n",
        "  ftrx[i]=xtrain[i].flatten();\n",
        "  ftrx[i]=ftrx[i]/256\n",
        "  ftry[i][ytrain[i]]=1\n",
        "for i in range(len(xtest)):\n",
        "  ftex[i]=xtest[i].flatten();\n",
        "  ftex[i]=ftex[i]/256\n",
        "  ftey[i][ytest[i]]=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     # activation functions and gradients\n",
        "\n",
        "def sigmoid(x):\n",
        "  return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\n",
        "\n",
        "def grad_sigmoid(z):\n",
        "  return z*(1-z)\n",
        "\n",
        "def softmax(z):\n",
        "  mx=np.max(z)\n",
        "  z=z-mx\n",
        "  out=(np.exp(z)+1e-9)/np.sum(np.exp(z)+1e-9)\n",
        "  return out\n",
        "\n",
        "def grad_softmax(yh,ty):\n",
        "  return yh-ty\n",
        "\n",
        "def Relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def grad_Relu(x):\n",
        "    return [1 if xi>0 else 0 for xi in x]\n",
        "\n",
        "def Tanh(x):\n",
        "  a= 2*np.where(x>=0, 1/(1+np.exp(-2*x)), np.exp(2*x)/(1+np.exp(2*x)))\n",
        "  return a-1\n",
        "\n",
        "\n",
        "def grad_Tanh(x):\n",
        "    return 1-Tanh(x)**2\n",
        "\n",
        "def InitParams(x,init):\n",
        "    weights=[]\n",
        "    baises=[]\n",
        "    if(init=='random'):\n",
        "      weights.append((np.random.randn(x[0],784)))\n",
        "      baises.append((np.random.randn(x[0])))\n",
        "      l=len(x)-1\n",
        "      for i in range(l):\n",
        "          weights.append(0.1*(np.random.randn(x[i+1],x[i])))\n",
        "          baises.append(0.1*(np.random.randn(x[i+1])))\n",
        "      weights.append(0.1*(np.random.randn(10,x[-1])))\n",
        "      baises.append(0.1*(np.random.randn(10)))\n",
        "    else:\n",
        "      weights.append(np.random.randn(x[0], 784)*np.sqrt(2./(784)))\n",
        "      baises.append(np.zeros(x[0]))\n",
        "      l=len(x)-1\n",
        "      for i in range(l):\n",
        "          weights.append(np.random.randn(x[i+1], x[i])*np.sqrt(2./(x[i])))\n",
        "          baises.append(np.zeros(x[i+1]))\n",
        "      weights.append(np.sqrt(2./(x[-1]))*(np.random.randn(10,x[-1])))\n",
        "      baises.append((np.zeros(10)))\n",
        "\n",
        "\n",
        "    return weights,baises\n",
        "\n",
        "\n",
        "\n",
        "def initgrads(N,Nunits):\n",
        "  dh,da,dw,db=[],[],[],[]\n",
        "  dw.append(np.zeros((Nunits[0],784)))\n",
        "  db.append(np.zeros(Nunits[0]))\n",
        "  da.append(np.zeros(Nunits[0]))\n",
        "  dh.append(np.zeros(Nunits[0]))\n",
        "  for i in range(1,N):\n",
        "    dw.append(np.zeros((Nunits[i],Nunits[i-1])))\n",
        "    db.append(np.zeros(Nunits[i]))\n",
        "    da.append(np.zeros(Nunits[i]))\n",
        "    dh.append(np.zeros(Nunits[i]))\n",
        "  dw.append(np.zeros((10,Nunits[-1])))\n",
        "  db.append(np.zeros(10))\n",
        "  da.append(np.zeros(10))\n",
        "  dh.append(np.zeros(10))\n",
        "  return dw,db,da,dh\n",
        "\n",
        "  \n",
        "def Feed_Frwd_Nw1(xTr,Weighs,Bais,activation,loss):\n",
        "  a_i=[]\n",
        "  h_i=[]\n",
        "  a_i.append(np.dot(Weighs[0],xTr)+Bais[0])       \n",
        "  if (activation=='sig'):\n",
        "    h_i.append(sigmoid(a_i[0]))\n",
        "  elif (activation=='tanh'):\n",
        "    h_i.append(Tanh(a_i[0]))\n",
        "  elif (activation=='relu'):\n",
        "    h_i.append(Relu(a_i[0]))\n",
        "  for i in range(1,len(Weighs)-1):\n",
        "    a_i.append((np.dot(Weighs[i],h_i[i-1])+Bais[i]))\n",
        "    if (activation=='sig'):\n",
        "      h_i.append(sigmoid(a_i[i]))\n",
        "    elif (activation=='tanh'):\n",
        "      h_i.append(Tanh(a_i[i]))\n",
        "    elif (activation=='relu'):\n",
        "      h_i.append(Relu(a_i[i]))\n",
        "  a_i.append(np.dot(Weighs[-1],h_i[-1])+Bais[-1])\n",
        "  h_i.append(softmax(a_i[-1])) \n",
        "  yp=h_i[-1]\n",
        "  return yp,a_i,h_i\n",
        "\n",
        "\n",
        "def Back_Prop(ip,ypr,ty,a_i,h_i,W,B,N,nrl,activation,Loss):\n",
        "  k=len(ypr)\n",
        "  dw,db,da,dh=initgrads(N,nrl)\n",
        "  if (Loss==\"ce\"):\n",
        "    dh[-1]=[-(t/ypr) if t==1 else 0 for t in ty ]\n",
        "    da[-1]=ypr-ty\n",
        "  elif (Loss==\"mse\"):\n",
        "    dh[-1]=(ypr-ty)\n",
        "    da[-1]=dh[-1]*(ypr-ypr**2)\n",
        "\n",
        "  db[-1]=da[-1] \n",
        "  dw[-1]=np.dot((da[-1][:,np.newaxis]),(h_i[-2][:,np.newaxis]).T)\n",
        " \n",
        "  for i in range(N-1,-1,-1):\n",
        "    dh[i]=np.squeeze(np.dot(W[i+1].T,da[i+1]))\n",
        "\n",
        "    if (activation=='sig'):\n",
        "      da[i]=dh[i]*grad_sigmoid(h_i[i])\n",
        "    elif (activation=='tanh'):\n",
        "      da[i]=dh[i]*grad_Tanh(a_i[i])\n",
        "    elif (activation=='relu'):\n",
        "      da[i]=dh[i]*grad_Relu(a_i[i])\n",
        "    db[i]=np.copy(da[i])\n",
        "    if (i==0):\n",
        "      dw[i]=np.dot(da[i][:,np.newaxis],ip[:,np.newaxis].T)\n",
        "    else:\n",
        "      dw[i]=np.dot(da[i][:,np.newaxis],h_i[i-1][:,np.newaxis].T)\n",
        "  return dw,db\n",
        "\n",
        "\n",
        "def sum_weights(W):\n",
        "  sum=0\n",
        "  for i in range(len(W)):\n",
        "    sum+=np.sum(W[i])\n",
        "  return sum\n",
        "\n",
        "\n",
        "\n",
        "           # Losses and accuracies\n",
        "def val_acc_loss(W,B,activation,ls_fun,wd):\n",
        "  cost=0\n",
        "  count=0\n",
        "  if (ls_fun=='ce'):\n",
        "    for i in range(len(ftrx[54000:])):\n",
        "      ypr,_,_=Feed_Frwd_Nw1(ftrx[i],W,B,activation,ls_fun)\n",
        "      cost += (np.sum(np.multiply(ftry[i],np.log(ypr))) + wd/2*sum_weights(W) )\n",
        "      if (np.argmax(ypr) ==np.argmax(ftry[i])):\n",
        "        count+=1\n",
        "    return -cost/6000, count/6000\n",
        "    #return np.sum(ay.multiply(np.log(yh)))\n",
        "  elif (ls_fun=='mse'):\n",
        "    for i in range(len(ftrx[54000:])):\n",
        "      ypr,_,_=Feed_Frwd_Nw1(ftrx[i],W,B,activation,ls_fun)\n",
        "      cost+=(0.5*np.sum((ftry[i]- ypr)**2) + wd/2*sum_weights(W) )\n",
        "      if (np.argmax(ypr) ==np.argmax(ftry[i])):\n",
        "        count+=1\n",
        "    return cost/6000, count/6000\n",
        "\n",
        "def train_acc_loss(W,B,activation,ls_fun,wd):\n",
        "  cost=0\n",
        "  count=0\n",
        "  if (ls_fun=='ce'):\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      ypr,_,_=Feed_Frwd_Nw1(ftrx[i],W,B,activation,ls_fun)\n",
        "      cost += (np.sum(np.multiply(ftry[i],np.log(ypr))) + wd/2*sum_weights(W) )\n",
        "      if (np.argmax(ypr) ==np.argmax(ftry[i])):\n",
        "        count+=1\n",
        "    return -cost/54000, count/54000\n",
        "    #return np.sum(ay.multiply(np.log(yh)))\n",
        "  elif (ls_fun=='mse'):\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      ypr,_,_=Feed_Frwd_Nw1(ftrx[i],W,B,activation,ls_fun)\n",
        "      cost+=(0.5*np.sum((ftry[i]- ypr)**2) + wd/2*sum_weights(W) )\n",
        "      if (np.argmax(ypr) ==np.argmax(ftry[i])):\n",
        "        count+=1\n",
        "    return cost/54000, count/54000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_acc(W,B,activation,ls_fun):\n",
        "  count=0\n",
        "  predictions=[]\n",
        "  matrix=np.zeros((10,10))\n",
        "  for i in range(len(ftex)):\n",
        "    ypr,_,_=Feed_Frwd_Nw1(ftex[i],W,B,activation,ls_fun)\n",
        "    if (np.argmax(ypr) == ytest[i]):\n",
        "      count+=1\n",
        "    predictions.append(np.argmax(ypr))\n",
        "    matrix[ytest[i]][np.argmax(ypr)]+=1\n",
        "    \n",
        "  lables={}\n",
        "  for i in range(len(items)):\n",
        "    lables[i]=items[i]\n",
        "  wandb.log({\"confusion matrix\": wandb.plot.confusion_matrix(probs=None,\n",
        "                                                              y_true=ytest,\n",
        "                                                              preds=predictions,\n",
        "                                                              class_names=items)})\n",
        "  wandb.log({\"confusion matrix v2.0\": wandb.plots.HeatMap(items, items, matrix, show_text=True),\"Accuracy\":count/len(ytest)})\n",
        "  return count/len(ytest)\n",
        "\n",
        "\n",
        "\n",
        "def gradDecent(ftrx,ftry,init,wd,eta,N,nrl,activation,Ls_fun,epochs,batchsize):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  e=0\n",
        "  while(epochs>0):\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1\n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun)\n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        for k in range(len(gw)):\n",
        "          W[k]=np.subtract(W[k],eta*uw[k]/batchsize)\n",
        "          B[k]=np.subtract(B[k],eta*ub[k]/batchsize)\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k]) \n",
        "            \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n",
        "\n",
        "\n",
        "import copy \n",
        "def Momentum(ftrx,ftry,init,batchsize,gamma,wd,eta,N,nrl,activation,Ls_fun,epochs=1):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  vw,vb,_,_=initgrads(N,nrl)\n",
        "  pw,pb,_,_=initgrads(N,nrl)\n",
        "  e=0\n",
        "  while(epochs>0):\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1\n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun)\n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        for k in range(len(gw)):\n",
        "          # z1,z2=W[k],B[k]\n",
        "          vw[k]=np.add(gamma*pw[k],eta*uw[k]/batchsize)\n",
        "          vb[k]=np.add(gamma*pb[k],eta*ub[k]/batchsize)\n",
        "          W[k]=np.subtract(W[k],vw[k])\n",
        "          B[k]=np.subtract(B[k],vb[k])\n",
        "          pw[k]=copy.deepcopy(vw[k])\n",
        "          pb[k]=copy.deepcopy(vb[k])\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k]) \n",
        "            \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n",
        "\n",
        "import copy \n",
        "def Nesterov(ftrx,ftry,init,batchsize,gamma,wd,eta,N,nrl,activation,Ls_fun,epochs=1):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  vw,vb,_,_=initgrads(N,nrl)\n",
        "  pw,pb,_,_=initgrads(N,nrl)\n",
        "  w,b=copy.deepcopy(W),copy.deepcopy(B)\n",
        "  e=0\n",
        "  while(epochs>0):\n",
        "    start = time.time()\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1 \n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx)):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun) \n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,w,b,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        for k in range(len(gw)):\n",
        "          # z1,z2=W[k],B[k]\n",
        "          vw[k]=np.add(gamma*pw[k],eta*uw[k]/batchsize)\n",
        "          vb[k]=np.add(gamma*pb[k],eta*ub[k]/batchsize)\n",
        "          W[k]=np.subtract(W[k],vw[k])\n",
        "          B[k]=np.subtract(B[k],vb[k])\n",
        "          pw[k]=copy.deepcopy(vw[k])\n",
        "          pb[k]=copy.deepcopy(vb[k])\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k])\n",
        "          for k in range(len(dw)):\n",
        "            w[k]=np.subtract(W[k],gamma*pw[k])\n",
        "            b[k]=np.subtract(B[k],gamma*pb[k])    \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n",
        "\n",
        "def rmsprop(ftrx,ftry,init,batchsize,eps,beta,wd,eta,N,nrl,activation,Ls_fun,epochs=1):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  vw,vb,_,_=initgrads(N,nrl)\n",
        "  e=0\n",
        "  while(epochs>0):\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1\n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun)\n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        for k in range(len(gw)):\n",
        "          vw[k]=np.add(beta*vw[k],(1-beta)*uw[k]**2)\n",
        "          vb[k]=np.add(beta*vb[k],(1-beta)*ub[k]**2)\n",
        "          W[k]=np.subtract(W[k],((eta/batchsize)/np.sqrt(vw[k]+eps))*uw[k])\n",
        "          B[k]=np.subtract(B[k],((eta/batchsize)/np.sqrt(vb[k]+eps))*ub[k])\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k]) \n",
        "            \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Adam(ftrx,ftry,init,batchsize,eps,beta1,beta2,wd,eta,N,nrl,activation,Ls_fun,epochs=1):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  vw,vb,_,_=initgrads(N,nrl)\n",
        "  mw,mb,_,_=initgrads(N,nrl)\n",
        "  beta1=0.9\n",
        "  e=0\n",
        "  t=1\n",
        "  while(epochs>0):\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1\n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun)\n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        t+=1\n",
        "        for k in range(len(gw)):\n",
        "          vw[k]=np.add(beta2*vw[k],(1-beta2)*uw[k]**2)\n",
        "          vb[k]=np.add(beta2*vb[k],(1-beta2)*ub[k]**2)\n",
        "\n",
        "          mw[k]=np.add(beta1*mw[k],(1-beta1)*uw[k])\n",
        "          mb[k]=np.add(beta1*mb[k],(1-beta1)*ub[k])\n",
        "          mw[k]/=(1-np.power(beta1,t,dtype=np.float64))\n",
        "          mb[k]/=(1-np.power(beta1,t,dtype=np.float64))\n",
        "          vw[k]=np.divide(vw[k],(1-np.power(beta2,t,dtype=np.float64)))\n",
        "          vb[k]=np.divide(vb[k],(1-np.power(beta2,t,dtype=np.float64)))\n",
        "          W[k]=np.subtract(W[k],((eta/batchsize)/np.sqrt(vw[k]+eps))*mw[k])\n",
        "          B[k]=np.subtract(B[k],((eta/batchsize)/np.sqrt(vb[k]+eps))*mb[k])\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k]) \n",
        "            \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def NAdam(ftrx,ftry,init,batchsize,eps,beta1,beta2,wd,eta,N,nrl,activation,Ls_fun,epochs=1):\n",
        "  W,B=InitParams(nrl,init)\n",
        "  vw,vb,_,_=initgrads(N,nrl)\n",
        "  mw,mb,_,_=initgrads(N,nrl)\n",
        "  beta1=0.9\n",
        "  e=0\n",
        "  t=1\n",
        "  while(epochs>0):\n",
        "    e+=1\n",
        "    start = time.time()\n",
        "    epochs-=1\n",
        "    uw,ub,_,_=initgrads(N,nrl)\n",
        "    for i in range(len(ftrx[:54000])):\n",
        "      pred_y,ai,hi=Feed_Frwd_Nw1(ftrx[i],W,B,activation,Ls_fun)\n",
        "      gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "      for j in range(len(gw)):\n",
        "        uw[j]=np.add(uw[j],gw[j]+wd*W[j])\n",
        "        ub[j]=np.add(ub[j],gb[j]+wd*B[j])\n",
        "      if((i+1)%batchsize==0 or i==53999):\n",
        "        t+=1\n",
        "        for k in range(len(gw)):\n",
        "          vw[k]=np.add(beta2*vw[k],(1-beta2)*uw[k]**2)\n",
        "          vb[k]=np.add(beta2*vb[k],(1-beta2)*ub[k]**2)\n",
        "\n",
        "          mw[k]=np.add(beta1*mw[k],(1-beta1)*uw[k])\n",
        "          mb[k]=np.add(beta1*mb[k],(1-beta1)*ub[k])\n",
        "          mw[k]/=(1-np.power(beta1,t,dtype=np.float64))\n",
        "          mb[k]/=(1-np.power(beta1,t,dtype=np.float64))\n",
        "          vw[k]=np.divide(vw[k],(1-np.power(beta2,t,dtype=np.float64)))\n",
        "          vb[k]=np.divide(vb[k],(1-np.power(beta2,t,dtype=np.float64)))\n",
        "          W[k]=np.subtract(W[k],((eta/batchsize)/np.sqrt(vw[k]+eps))*((beta1*mw[k]+(1-beta1)*uw[k])/(1-beta1**t)))\n",
        "          B[k]=np.subtract(B[k],((eta/batchsize)/np.sqrt(vb[k]+eps))*((beta1*mb[k]+(1-beta1)*ub[k])/(1-beta1**t)))\n",
        "          uw[k]=np.subtract(uw[k],uw[k])\n",
        "          ub[k]=np.subtract(ub[k],ub[k]) \n",
        "            \n",
        "    end = time.time() \n",
        "    tloss,tacc=train_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    vloss,vacc=val_acc_loss(W,B,activation,Ls_fun,wd)\n",
        "    wandb.log({\"epoch\":e,\"Train_loss\":tloss,\"Train_acc\":tacc,\"val_loss\":vloss,\"val_Accuracy\":vacc})\n",
        "    #print(\"epochs :\",e,\"train_loss :\",tloss,\"  Train_acc:\",tacc,\"val_loss:\",vloss,\"val_Accuracy:\",vacc,\"    time:\",math.ceil(end-start))\n",
        "  test_acc(W,B,activation,Ls_fun)  \n",
        "  return hi,W,B\n"
      ],
      "metadata": {
        "id": "iDdWe-f_Wfv9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nrl=[32,64] # number of neurons in each hidden layer\n",
        "N=len(nrl)\n",
        "hi,W,B=Adam(ftrx,ftry,'xaviour',64,1e-8,0.9,0.999,0.0,0.1,N,nrl,'sig','ce',10)"
      ],
      "metadata": {
        "id": "SsVbXfmmyIjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=[1e-3,1e-4,1e-2,1e-5,2e-3,2e-4,2e-2,2e-5,3e-3,3e-4,3e-2,3e-5,4e-3,4e-4,4e-2,4e-5,5e-3,5e-4,5e-2,5e-5,6e-3,6e-4,6e-2,6e-5,7e-3,7e-4,7e-2,7e-4,8e-3,8e-4,8e-2,8e-5,9e-3,9e-4,9e-2,9e-5,1e-1,2e-1,3e-1,4e-1,5e-1,6e-1,1.5e-3,1.5e-4,1.5e-2,1.5e-5,2.5e-3,2.5e-4,2.5e-2,2.5e-5,3.5e-3,3.5e-4,3.5e-2,3.5e-5,4.5e-3,4.5e-4,4.5e-2,4.5e-5,5.5e-3,5.5e-4,5.5e-2,5.5e-5,6.5e-3,6.5e-4,6.5e-2,6.5e-5,7.5e-3,7.5e-4,7.5e-2,7.5e-4,8.5e-3,8.5e-4,8.5e-2,8.5e-5,9.5e-3,9.5e-4,9.5e-2,9.5e-5,1.5e-1,2.5e-1,3.5e-1,4.5e-1,5e-1,6.5e-1]\n",
        "len(lr)"
      ],
      "metadata": {
        "id": "COwRdmRZyZDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val=[0.065, 0.06, 0.25, 0.35, 0.00045, 0.00055, 5.5e-06, 0.0001, 1.5e-05, 0.045, 0.05, 0.0035, 6.5e-05, 0.04, 0.0007, 0.00075, 0.035, 0.095, 0.1, 0.0008, 0.0003, 0.025, 4.5e-05, 0.00035, 0.02, 0.085, 0.015, 0.075, 0.08, 0.0009, 9.5e-05, 0.0004, 1e-05, 0.01, 0.009, 0.0095, 0.0085, 0.0075, 0.008, 0.00095, 0.0055, 0.006, 0.0045, 0.005, 0.004, 0.007, 0.003, 0.0025, 0.002, 0.0015, 0.0065]\n",
        "print(val)"
      ],
      "metadata": {
        "id": "Z2Z0uAyRIAny",
        "outputId": "2a4f8516-23be-42fe-c198-365347fdd56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.065, 0.06, 0.25, 0.35, 0.00045, 0.00055, 5.5e-06, 0.0001, 1.5e-05, 0.045, 0.05, 0.0035, 6.5e-05, 0.04, 0.0007, 0.00075, 0.035, 0.095, 0.1, 0.0008, 0.0003, 0.025, 4.5e-05, 0.00035, 0.02, 0.085, 0.015, 0.075, 0.08, 0.0009, 9.5e-05, 0.0004, 1e-05, 0.01, 0.009, 0.0095, 0.0085, 0.0075, 0.008, 0.00095, 0.0055, 0.006, 0.0045, 0.005, 0.004, 0.007, 0.003, 0.0025, 0.002, 0.0015, 0.0065]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_config = {\n",
        "                  'method'    : \"random\",\n",
        "                  'metric'    : {'name': 'val_Accuracy','goal':'maximize'},\n",
        "                  'parameters': {\n",
        "                                  'optimizer'              : {'values': ['sgd','gd','momentum','nesterov','rmsprop','adam','nadam']},\n",
        "                                  'hidden_layer_size'        : {'values':[32, 64,16,20,128,50]},\n",
        "                                  'num_hidden_layers'                  : {'values':[1,2,3,5,7,8] },\n",
        "                                  'activation'         : {'values':['sig','relu','tanh']},\n",
        "                                  'lossfun'            : {'values':['ce']},\n",
        "                                  'lr'       : {'values':[0.065, 0.06, 0.25, 0.35, 0.00045, 0.00055, 5.5e-06, 0.0001, 1.5e-05, 0.045, 0.05, 0.0035, 6.5e-05, 0.04, 0.0007, 0.00075, 0.035, 0.095, 0.1, 0.0008, 0.0003, 0.025, 4.5e-05, 0.00035, 0.02, 0.085, 0.015, 0.075, 0.08, 0.0009, 9.5e-05, 0.0004, 1e-05, 0.01, 0.009, 0.0095, 0.0085, 0.0075, 0.008, 0.00095, 0.0055, 0.006, 0.0045, 0.005, 0.004, 0.007, 0.003, 0.0025, 0.002, 0.0015, 0.0065]},\n",
        "                                   'weights_initializer': {'values':['random','Xavier']},\n",
        "                                  'epochs'             :{'values':[5,10,6,8,15]},\n",
        "                                  'weightDecay'        :{'values':[0.005,0,0.00005,0.0005,0.0006]},\n",
        "                                  'batchsize'         :{'values':[16,32,64,128]},\n",
        "\n",
        "                                }\n",
        "                }"
      ],
      "metadata": {
        "id": "pb0EmVSqAbQ6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "  hyperparameter=dict(\n",
        "      epochs = 5,                                       \n",
        "      batchsize = 10,                                                    \n",
        "      weightDecay = 0.0005,                                            \n",
        "      noOfNeurons = 32,\n",
        "      layerInitialization='random',                               \n",
        "      learningrate=1e-3,\n",
        "      activation='sig',                                             \n",
        "      optim = \"momentum\",                      \n",
        "      N= 3,           \n",
        "      Lossfun = \"ce\",\n",
        "      \n",
        "  )\n",
        "\n",
        "  wandb.init(config=hyperparameter)\n",
        "\n",
        "  config=wandb.config\n",
        "  wandb.run.name = \"e_{}_hls_{}_numhl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}_learning_rate_{}_wdecay_{}\".format(config.epochs,\\\n",
        "                                                                      config.hidden_layer_size,\\\n",
        "                                                                      config.num_hidden_layers,\\\n",
        "                                                                      config.optimizer,\\\n",
        "                                                                      config.batchsize,\\\n",
        "                                                                      config.weights_initializer,\\\n",
        "                                                                      config.activation,\\\n",
        "                                                                      config.lossfun,\\\n",
        "                                                                      config.lr,\n",
        "                                                                      config.weightDecay)\n",
        "  output_size=10                                      \n",
        "  nrl=[config.noOfNeurons for i in range(config.N)]   #hidden layer sizes array creation\n",
        "  if(config.optim==\"sgd\"):\n",
        "    param=gradDecent(ftrx,ftry,config.weights_initializer,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs,1)\n",
        "  if(config.optim==\"gd\"):\n",
        "    param=gradDecent(ftrx,ftry,config.weights_initializer,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs,config.batchsize)\n",
        "  elif(config.optim==\"adam\"):\n",
        "    param=Adam(ftrx,ftry,config.weights_initializer,config.batchsize,1e-8,0.9,0.9,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs)\n",
        "  elif(config.optim==\"nadam\"):\n",
        "    param=NAdam(ftrx,ftry,config.weights_initializer,config.batchsize,1e-8,0.9,0.9,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs)\n",
        "  elif(config.optim==\"momentum\"):\n",
        "    param=Momentum(ftrx,ftry,config.weights_initializer,config.batchsize,0.9,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs)\n",
        "  elif(config.optim==\"nesterov\"):\n",
        "    param=Nesterov(ftrx,ftry,config.weights_initializer,config.batchsize,0.9,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs)\n",
        "  elif(config.optim==\"rmsprop\"):\n",
        "    param=rmsprop(ftrx,ftry,config.weights_initializer,config.batchsize,1e-8,0.8,config.weightDecay,config.lr,config.N,nrl,config.activation,config.lossfun,config.epochs)"
      ],
      "metadata": {
        "id": "QPFDszOXILuz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(s_config,project='CS6910-assignment-1')\n",
        "wandb.agent(sweep_id,train,project='CS6910-assignment-1',count=150)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bZWg7ukx0ZkS",
        "outputId": "2ef909b6-f605-4456-921f-48c9bbf9c04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ycbhw7g9\n",
            "Sweep URL: https://wandb.ai/pratap49/CS6910-assignment-1/sweeps/ycbhw7g9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d4zdmuad with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlossfun: ce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweightDecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweights_initializer: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratap49\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/pratap49/CS6910-assignment-1/runs/d4zdmuad\" target=\"_blank\">bright-sweep-1</a></strong> to <a href=\"https://wandb.ai/pratap49/CS6910-assignment-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/pratap49/CS6910-assignment-1/sweeps/ycbhw7g9\" target=\"_blank\">https://wandb.ai/pratap49/CS6910-assignment-1/sweeps/ycbhw7g9</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}