{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_J-lF6nI6yF",
        "outputId": "02e65e15-8f3e-4897-ea8e-b108bf6b97a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1RpThqibPC3Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random,os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP2V08jKObnr",
        "outputId": "0477aeff-b495-467a-cc00-b45299af63fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DSy_F7t4Ocrz"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "C5fBrswxI_c4"
      },
      "outputs": [],
      "source": [
        "# Imported the excel data file from the drive\n",
        "\n",
        "data = pd.read_excel(\"/content/Backstreet_Boys_Lyrics_score.xlsx\")\n",
        "\n",
        "# Stored the title and the lyrics of the song\n",
        "\n",
        "data = data[['Title','Lyrics']]\n",
        "\n",
        "#print(data.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fcSfLPC-NZE1"
      },
      "outputs": [],
      "source": [
        "class Generating_data(Dataset):  \n",
        "    def __init__(self, cntrl_code, type_of_gpt2=\"gpt2\", maximum_length=1024):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(type_of_gpt2)\n",
        "        #declaring the array for the lyrics\n",
        "        self.song_lyrics = []\n",
        "\n",
        "        #Here converting the each lyric into the tensor\n",
        "        for row in data['Lyrics']:\n",
        "          self.song_lyrics.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{cntrl_code}|>{row[:maximum_length]}<|endoftext|>\")\n",
        "            ))               \n",
        "      \n",
        "      #storing the length of the song_lyrics in song_lyrics_count\n",
        "        self.song_lyrics_count = len(self.song_lyrics)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.song_lyrics_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.song_lyrics[item]\n",
        "    \n",
        "dataset = Generating_data(data['Lyrics'], type_of_gpt2=\"gpt2\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5vxYINgANiE6"
      },
      "outputs": [],
      "source": [
        "#Importing the GPT-2 Model\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "#Importing the GPT-2 tokenizer\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "def pack_tensor(new_tensor, packed_tensor, maximum_seq_length):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > maximum_seq_length:\n",
        "        return packed_tensor, False, new_tensor\n",
        "        \n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BJtmSZvCNi--"
      },
      "outputs": [],
      "source": [
        "#training function \n",
        "def train(dataset, gpt_model, gpt_tokenizer, batch_size=36, no_of_epochs=15, learning_rate=1e-4, maximum_seq_length=400, warmup_steps=250, type_of_gpt2=\"gpt2\", output_directory=\".\", output_prefix=\"wreckgar\", test_mode=False):\n",
        "    acc_steps = 100\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpt_model = gpt_model.cuda()\n",
        "    gpt_model.train()\n",
        "\n",
        "    #AdamW optimizer is used\n",
        "    optimizer = AdamW(gpt_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    #schedules\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1)\n",
        "\n",
        "    #Loading the training data and storing it into the train_dataloader\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    #Initial values for loss, acc_batch_count are 0 & input_tensor set None\n",
        "    loss=0\n",
        "    acc_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(no_of_epochs):\n",
        "\n",
        "        #printing the training epoch\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        #printing the loss value\n",
        "        print(loss)\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = gpt_model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (acc_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                gpt_model.zero_grad()\n",
        "\n",
        "            acc_batch_count =acc_batch_count + 1\n",
        "            input_tensor = None\n",
        "        \n",
        "    return gpt_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emaEPedONpWM",
        "outputId": "04d49862-ed40-4bc6-d0cb-1a1b3d9f4a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "tensor(3.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 2\n",
            "tensor(3.1025, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 3\n",
            "tensor(2.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 4\n",
            "tensor(3.1287, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 5\n",
            "tensor(3.1741, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 6\n",
            "tensor(2.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 7\n",
            "tensor(2.4608, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 8\n",
            "tensor(3.0630, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 9\n",
            "tensor(2.6722, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 10\n",
            "tensor(2.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 11\n",
            "tensor(2.9942, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 12\n",
            "tensor(2.4116, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 13\n",
            "tensor(2.7856, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 14\n",
            "tensor(2.4670, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "109it [00:09, 11.61it/s]\n"
          ]
        }
      ],
      "source": [
        "gpt_model = train(dataset, gpt_model, gpt_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eXGIPFh7Nsoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f424751-7462-498e-82b1-41b6f4be6568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n"
          ]
        }
      ],
      "source": [
        "#Here is the function which generates the lyrics based on the given input\n",
        "#generate function will multiple input arguments like: gpt_model, gpt_tokenizer, prompt...and so on\n",
        "\n",
        "def generate(gpt_model, gpt_tokenizer, prompt, entry_count=10, entry_length=20, top_p=0.8, temperature=1):\n",
        "    gpt_model.eval()\n",
        "    #initial values\n",
        "    genertd_num = 0\n",
        "    genertd_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            genertd = torch.tensor(gpt_tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = gpt_model(genertd, labels=genertd)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                genertd= torch.cat((genertd, next_token), dim=1)\n",
        "\n",
        "                if next_token in gpt_tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "                  #incrementing the generated_num by 1\n",
        "                    genertd_num = genertd_num + 1\n",
        "                  #append output_text into the generated_list\n",
        "                    output_list = list(genertd.squeeze().numpy())\n",
        "                    output_text = gpt_tokenizer.decode(output_list)\n",
        "                    genertd_list.append(output_text)\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(genertd.squeeze().numpy())\n",
        "              output_text = gpt_tokenizer.decode(output_list)\n",
        "              genertd_list.append(output_text)\n",
        "                \n",
        "    return genertd_list\n",
        "\n",
        "song_lyrics = []\n",
        "\n",
        "#Here running the function multiple times to generate the multiple lyrics for the same given input \n",
        "\n",
        "for i in range(10):\n",
        "  song_lyrics.append(generate(gpt_model.to('cpu'), gpt_tokenizer, \"I love deep learning\", entry_count=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Fu4LqBztdZsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7cf3f9a-987f-47af-d3b6-c6001da7c0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love deep learning for both scientific and social science. I work for @Googlego for a couple years and live in']\n",
            "[\"I love deep learning. It's my whole body, I always love it. I always love it. My mind goes\"]\n",
            "[\"I love deep learning, so I think it's important to understand it, but not what you do.\\n\\nFirst\"]\n",
            "['I love deep learning.\\n\\nQ: Why are you saying this to you?\\n\\nA: The magic number']\n",
            "['I love deep learning and are used to working with 3rd party tools such as Caffe, SQL, etc. It']\n",
            "[\"I love deep learning, but I know that will be a challenge. I think it's time to do something about it\"]\n",
            "[\"I love deep learning and Machine Learning so much!\\n\\nAnd lastly, I'd like to bring a few words\"]\n",
            "['I love deep learning and want to help you to make better decisions for yourself and your family.\\n\\nBest Practices for']\n",
            "['I love deep learning, so I had to start by asking you about the one and only approach for training deep learning that']\n",
            "[\"I love deep learning in general. I've been working on this stuff for over 10 years, and I think it's\"]\n"
          ]
        }
      ],
      "source": [
        "# Here printing the  generated lyrics\n",
        "\n",
        "for lyric in song_lyrics:\n",
        "    print(lyric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0NFPp3sjN8pN"
      },
      "outputs": [],
      "source": [
        "#Here stores the generated lyrics in the file.\n",
        "\n",
        "f = open(\"generated_lyrics_out.txt\",'w')\n",
        "#saving the generated lyrics in the generated_lyrics_out.txt file\n",
        "\n",
        "for lyric in song_lyrics:\n",
        "    f.write(lyric[0])\n",
        "    f.write(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GPT2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}